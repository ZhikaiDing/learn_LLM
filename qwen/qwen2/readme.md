# 1. ä¸‹è½½å¹¶ç®€å•è°ƒç”¨
## 1.1 èµ„æº
- æ¨¡å‹èµ„æº

  https://huggingface.co/Qwen/Qwen2-7B-Instruct/tree/main

- è„šæœ¬æ ·ä¾‹

  https://github.com/QwenLM/Qwen2

- æ–‡æ¡£

  https://qwenlm.github.io/zh/blog/qwen2/

  https://qwen.readthedocs.io/en/latest/

## 1.2 ç®€å•æµ‹è¯•
- åŠ è½½ torch æ¨¡å‹, å¹¶ç®€å•ä½¿ç”¨
  fast_start/simple_try_0.py

- å¿«é€Ÿåˆ›å»ºæœåŠ¡, å¹¶è¯·æ±‚
  - åˆ›å»ºæœåŠ¡: fast_start/simple_try_create_service.py
  - è¯·æ±‚: fast_start/try_call_service.py
  - è¯·æ±‚åè¾“å‡ºæ—¥å¿—: fast_start/fast_api_svc.log

## 1.3 æµ‹è¯• function calling


# 2. SFT

å‚è€ƒæ–‡æ¡£: https://github.com/QwenLM/Qwen2/blob/main/docs/source/training/SFT/example.rst
    æˆ– https://qwen.readthedocs.io/en/latest/training/SFT/example.html

## 2.0 ä¾èµ–

pip install peft deepspeed optimum accelerate

## 2.1 æ•°æ®æ ¼å¼
- å‚è€ƒæ ¼å¼ - jsonl
  - å‚è€ƒæ•°æ®: finetune/io_file/input_data/example_data.jsonl
  - è¯´æ˜
    - æ¯è¡Œæ˜¯ä¸€ä¸ª json æ•°æ®
    - json æ•°æ®
      - æ™®é€šæ•°æ®

        finetune/io_file/input_data/example_one_data.json

      - function calling æ•°æ® ???

        finetune/io_file/input_data/example_one_functioncall_data.json - **empty now**

- æ·±å…¥è§£æ
  - è§’è‰²
    - system: ç³»ç»Ÿçº§promptè¾“å…¥
    - user: ç”¨æˆ·è¾“å…¥
    - assistant: æ¨¡å‹å›å¤

  - æ¯ä¸ªè§’è‰²çš„æ–‡æœ¬çš„å¤´å°¾æ ‡è®°
    - å¤´: <|im_start|>

      æ³¨: åé¢ç´§è·Ÿ è§’è‰²+å›è½¦, å¦‚ "<|im_start|>system\n"

    - å°¾: <|im_end|>

      æ³¨: åé¢ç´§è·Ÿå›è½¦\n, å¦‚ "<|im_end|>\n"

  - å¤šè½®å¯¹è¯ç»“å°¾

    ä¸ºäº†å¼•å¯¼æ¨¡å‹è¿›è¡Œè¾“å‡º, ä¸€èˆ¬ä»¥ <|im_start|>assistant\n ç»“å°¾

    - eg
      ```
      <|im_start|>system
      ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ™ºèƒ½åŠ©æ‰‹<|im_end|>
      <|im_start|>user
      ä½ å¥½<|im_end|>
      <|im_start|>assistant
      
      ```

  - å…³äºç‰¹æ®Šçš„ token æ ‡è®°
    æŸ¥çœ‹æ¨¡å‹ä»“åº“(Qwen/Qwen2-7B-Instruct)ä¸­çš„æ–‡ä»¶: tokenizer_config.json

    - è¡¥å……: chat æ¨¡ç‰ˆ
      - å­—æ®µå†…å®¹

        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"

      - å«ä¹‰

        tokenizer_config.json ä¸­çš„ "chat_template" å­—æ®µå®šä¹‰äº†å¦‚ä½•å°†èŠå¤©æ¶ˆæ¯åºåˆ—åŒ–æˆæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
        è¿™é‡Œçš„æ¨¡æ¿ä½¿ç”¨äº† Jinja2 æ¨¡æ¿å¼•æ“çš„è¯­æ³•ï¼Œå®ƒæ˜¯ä¸€ç§ç”¨äºç”ŸæˆåŠ¨æ€å†…å®¹çš„æ¨¡æ¿è¯­è¨€ã€‚

        åœ¨è¿™ä¸ªç‰¹å®šçš„æ¨¡æ¿ä¸­ï¼Œ{% for message in messages %} è¡¨ç¤ºéå† messages åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«è§’è‰²å’Œå†…å®¹çš„æ¶ˆæ¯å­—å…¸ï¼‰ã€‚æ¥ä¸‹æ¥çš„ {% if %} å’Œ {% endif %} æ˜¯æ¡ä»¶è¯­å¥ï¼Œç”¨äºæ£€æŸ¥ç¬¬ä¸€ä¸ªæ¶ˆæ¯æ˜¯å¦æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™ä¼šæ·»åŠ ä¸€æ®µé»˜è®¤çš„ç³»ç»Ÿä¿¡æ¯ã€‚

  - æ›´å¤š token

    æŸ¥çœ‹æ¨¡å‹ä»“åº“(Qwen/Qwen2-7B-Instruct)ä¸­çš„æ–‡ä»¶: tokenizer.json

## 2.2 è„šæœ¬ & å‚æ•°è¯´æ˜
### 2.2.1 è„šæœ¬ä½ç½®

ç”± finetune/finetune.sh è°ƒç”¨ finetune/finetune.py

### 2.2.2 å‚æ•°è¯´æ˜
- finetune/finetune.sh

**å®˜æ–¹è¯´æ˜**

è¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒï¼ˆæˆ–å• GPU è®­ç»ƒï¼‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·æŒ‡å®šä»¥ä¸‹å˜é‡ï¼šGPUS_PER_NODE , NNODES , NODE_RANK , MASTER_ADDR å’Œ MASTER_PORT ã€‚

æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†é»˜è®¤è®¾ç½®ï¼Œå› æ­¤æ— éœ€è¿‡å¤šæ‹…å¿ƒã€‚åœ¨å‘½ä»¤ä¸­ï¼Œä½ å¯ä»¥é€šè¿‡å‚æ•° -m å’Œ -d åˆ†åˆ«æŒ‡å®šæ¨¡å‹è·¯å¾„å’Œæ•°æ®è·¯å¾„ã€‚

æ‚¨è¿˜å¯ä»¥é€šè¿‡å‚æ•° --deepspeed æŒ‡å®š deepspeed é…ç½®æ–‡ä»¶ã€‚
æˆ‘ä»¬ä¸º ZeRO2 å’Œ ZeRO3 æä¾›äº†ä¸¤ä¸ªé…ç½®æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©å…¶ä¸­ä¸€ä¸ªã€‚
å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ ZeRO3 è¿›è¡Œå¤š GPU è®­ç»ƒï¼Œä½† Q-LoRA é™¤å¤–ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ ZeRO2ã€‚

  - å˜é‡ â˜…

    BASE_PATH - åŸºç¡€è·¯å¾„, finetune æ–‡ä»¶å¤¹çš„è·¯å¾„
    
    MODEL - æ¨¡å‹åç§° | æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
    
    DATA - æ•°æ® ç›¸å¯¹è·¯å¾„
    
    DS_CONFIG_PATH - deepspeed é…ç½®æ–‡ä»¶ ç›¸å¯¹è·¯å¾„
    
    USE_LORA - æ˜¯å¦é‡‡ç”¨ LoRA è®­ç»ƒ
    
    Q_LORA - æ˜¯å¦é‡‡ç”¨ Q-LoRA è®­ç»ƒ
    
    OUTPUT_DIR - è¾“å‡ºä½ç½®

  - å‘½ä»¤è¡Œå‚æ•°

    --bf16 - æ˜¯å¦é‡‡ç”¨ bfloat16 ç²¾åº¦è®­ç»ƒ(åŠç²¾åº¦è®­ç»ƒ)
    
    --num_train_epochs - è®­ç»ƒæ•°æ®çš„epochè½®æ•° â˜…
    
    --per_device_train_batch_size - æ¯ä¸ª GPU ç”¨äºè®­ç»ƒçš„æ‰¹æ¬¡å¤§å° â˜…
    
    --per_device_eval_batch_size - æ¯ä¸ª GPU ç”¨äºè¯„ä¼°çš„æ‰¹æ¬¡å¤§å°
    
    --gradient_accumulation_steps - æ¢¯åº¦ç´¯ç§¯æ­¥æ•° â˜…
    
    --eval_strategy - è¦ä½¿ç”¨çš„è¯„ä¼°ç­–ç•¥ | old: evaluation_strategy
    
    --save_strategy - è¦ä½¿ç”¨çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥ - transformers.TrainingArguments
    
    --save_steps - æ¯Xä¸ªæ›´æ–°æ­¥éª¤ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ - transformers.TrainingArguments
    
    --save_total_limit - é™åˆ¶æ£€æŸ¥ç‚¹çš„æ€»æ•°,åˆ é™¤ä¸­çš„æ—§æ£€æŸ¥ç‚¹ - transformers.TrainingArguments
    
    --learning_rate - å­¦ä¹ ç‡ â˜†
    
    --weight_decay - æƒé‡è¡°å‡å€¼ â˜†
    
    --adam_beta2 - Adamw ä¼˜åŒ–å™¨çš„ Î²2 å€¼ â˜†
    
    --warmup_ratio - çº¿æ€§é¢„çƒ­å æ€»æ­¥éª¤çš„æ¯”ä¾‹ â˜†
    
    --lr_scheduler_type - å­¦ä¹ ç‡å˜åŒ–æ–¹å¼ â˜†
    
    --logging_steps - æ¯Xæ¬¡ä¿å­˜æ—¥å¿—
    
    --report_to - è¦å‘å…¶æŠ¥å‘Šç»“æœå’Œæ—¥å¿—çš„é›†æˆåˆ—è¡¨
    
    --model_max_length - æœ€å¤§åºåˆ—é•¿åº¦ â˜…
    
    --lazy_preprocess - æ˜¯å¦é‡‡ç”¨ LazySupervisedDataset, å¦åˆ™é‡‡ç”¨ SupervisedDataset
    
    --gradient_checkpointing - æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

    **æ³¨**: 

    (1) æ€»æ‰¹æ¬¡å¤§å°ç­‰äº per_device_train_batch_size Ã— num_of_gpu Ã— gradient_accumulation_steps
    
    (2) æ³¨æ„å‘½ä»¤è¡Œå‚æ•°: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead - transformers.TrainingArguments
    
    (3) è¾“å…¥ --bf16 æˆ– --fp16 å¯ä»¥æŒ‡å®šæ··åˆç²¾åº¦è®­ç»ƒçš„ç²¾åº¦

  - finetune.py å†…éƒ¨å‚æ•°
    - LoraArguments

      lora_r - LoRA çš„ç§© â˜…
      
      lora_alpha - LoRA çš„ç¼©æ”¾ç³»æ•° â˜†
      
      lora_dropout - LoRA çš„ dropout ç‡
      
      lora_target_modules - è¦ç”¨LoRAæ›¿æ¢çš„æ¨¡å—åç§°åˆ—è¡¨æˆ–æ¨¡å—åç§°çš„æ­£åˆ™è¡¨è¾¾å¼
      
      lora_weight_path - LoRA æƒé‡è·¯å¾„ ???
      
      lora_bias - Loraçš„åå·®ç±»å‹, å¯ä»¥æ˜¯: 'none'|'all'|'lora_only'

## 2.3 è¾“å‡º
